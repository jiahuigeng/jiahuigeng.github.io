---
title: "Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models"
collection: publications
category: conferences
permalink: /publication/2025-03-04-shaping-safety-boundaries
excerpt: 'This paper explores methods for understanding and defending against jailbreak attacks in large language models.'
date: 2025-03-04
venue: 'ACL 2025 Main'
paperurl: '#'
citation: 'L Gao, J Geng, X Zhang, P Nakov, X Chen. (2025). &quot;Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models.&quot; <i>ACL 2025 Main</i>.'
---

This work provides comprehensive insights into jailbreak attacks on large language models and proposes effective defense mechanisms to enhance AI safety and security.