---
title: "Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities"
collection: publications
category: conferences
permalink: /publication/2025-03-02-con-instruction
excerpt: 'This paper presents a universal jailbreaking method for multimodal large language models using non-textual modalities.'
date: 2025-03-02
venue: 'ACL 2025 Main'
paperurl: '#'
citation: 'J Geng, TT Tran, P Nakov, I Gurevych. (2025). &quot;Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities.&quot; <i>ACL 2025 Main</i>.'
---

This work demonstrates how multimodal large language models can be universally jailbroken through carefully crafted non-textual inputs, highlighting critical security concerns in multimodal AI systems.